---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently pursuing a Ph.D. (DoGS SPRING Fellowship) at the [Graduate School of Informatics](https://www.i.kyoto-u.ac.jp/), [Kyoto University](https://www.kyoto-u.ac.jp/), Japan. 
I am a member of the [LSTA Lab.](https://www.lsta.media.kyoto-u.ac.jp/), where I conduct research on document and text analysis, recognition, and processing. 
If you are interested in any form of academic collaboration, please feel free to [email](mailto:jryjry1094791442@gmail.com) me.

I received my Master of Science (M.S.) degree in 2025 from the [Graduate Institute of Networking and Multimedia](https://www.inm.ntu.edu.tw/), [National Taiwan University](https://www.ntu.edu.tw/), Taipei, Taiwan.
I was a member of [NTU imLab](https://ntuimlab.tw/), and my master‚Äôs thesis focused on 3D Gaussian Splatting (3DGS) and 3D head reconstruction.

I obtained my Bachelor of Science (B.S.) degree in 2023 in [Electrical and Computer Engineering](http://www.ee.tku.edu.tw/) from [Tamkang University](https://www.tku.edu.tw/), New Taipei City, Taiwan, where I ranked first in my department. 
I conducted my undergraduate research at the Advanced Mixed-Operation System Laboratory (AMOS Lab.) at Tamkang University, focusing on image processing and computer vision.

My primary research interests include Natural Language Processing (i.e., VLMs and LLMs), Computer Vision (i.e., document binarization, image generation, super-resolution, object detection, and optical character recognition), and Computer Graphics (3D Gaussian Blendshapes).
I have published numerous papers <a href='https://scholar.google.com/citations?user=r8F35p8AAAAJ'><img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2Fruiyangju%2Fruiyangju.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=Citations"></a> in international journals or conferences.

I am highly active in international academic activities and currently serve as a reviewer for several international journals, <a href="https://orcid.org/0000-0003-2240-1377"><img src="https://img.shields.io/endpoint?url=https://cdn.jsdelivr.net/gh/ruiyangju/ruiyangju.github.io@main/orcid_peer_review_shieldsio.json&logo=ORCID&labelColor=f6f6f6&color=9cf&style=flat&label=Peer%20Reviews"></a> including IEEE TVCG, PR, KBS, and NN, as well as for international conferences such as AAAI, ICASSP, and IJCNN.

<!--
Further details about my background are available in <a href='https://ruiyangju.github.io/images/CV_English.pdf'><img src='https://img.shields.io/badge/Resume-English-white?color=green'></a> (updated in July 2025).
-->

<span class="anchor" id="news"></span>

# üì¢ News
<div style="height:200px; width: fit-content; overflow-y: auto; background:#FFFFFF; padding: 5px; border-radius: 6px; border: 1px solid #ccc;">
<ul>
  <li>2026.01: Two papers are accepted by <strong>IEEE VR 2026</strong> [Poster].</li>
  <li>2026.01: One paper is accepted by <strong>ICASSP 2026</strong>.</li>
  <li>2025.12: One paper is accepted by <strong>IET Image Processing</strong> (<a href="https://github.com/RuiyangJu/FCE-YOLOv8" target="_blank">GitHub</a>).</li>
  <li>2025.11: I receive the <strong>Kyoto University DoGS SPRING Fellowship</strong> for my Phd studies.</li>
  <li>2025.08: One paper is accepted by <strong>APSIPA ASC 2025</strong> [Oral] (<a href="https://github.com/RuiyangJu/Efficient_Document_Image_Binarization" target="_blank">GitHub</a>).</li>
  <li>2025.08: One paper is accepted by <strong>ADMA 2025</strong> [Short Paper].</li>
  <li>2025.06: I receive an offer for a PhD student at <strong>Kyoto University</strong>, Japan.</li>
  <li>2025.06: My master's thesis project, <strong>ToonifyGB</strong>, is now available (<a href="https://ruiyangju.github.io/ToonifyGB" target="_blank">Project</a>) (<a href="https://github.com/RuiyangJu/ToonifyGB" target="_blank">GitHub</a>).</li>
  <li>2025.03: One paper is accepted by <strong>IEEE Access</strong> (<a href="https://github.com/RuiyangJu/Fracture_Detection_Improved_YOLOv8" target="_blank">GitHub</a>).</li>
  <li>2025.01: One paper is accepted by <strong>ICRA 2025</strong> [Oral] (<a href="https://www.neiljin.site/projects/orbsfm" target="_blank">Project</a>) (<a href="https://github.com/PeaceNeil/ORB-SfMLearner" target="_blank">GitHub</a>) .</li>
  <li>2024.09: One paper is accepted by <strong>Knowledge-Based Systems</strong> (<a href="https://github.com/abcpp12383/ThreeStageBinarization" target="_blank">GitHub</a>).</li>
  <li>2024.08: One paper is accepted by <strong>ICONIP 2024</strong> [Oral] (<a href="https://github.com/RuiyangJu/Fracture_Detection_Improved_YOLOv8" target="_blank">GitHub</a>).</li>
  <li>2024.05: One paper is accepted by <strong>Electronics Letters</strong> (<a href="https://github.com/RuiyangJu/YOLOv9-Fracture-Detection" target="_blank">GitHub</a>).</li>
  <li>2023.11: One paper is accepted by <strong>Scientific Reports</strong> (<a href="https://github.com/RuiyangJu/Bone_Fracture_Detection_YOLOv8" target="_blank">GitHub</a>).</li>
  <li>2023.08: One paper is accepted by <strong>PRICAI 2023</strong> [Oral] (<a href="https://github.com/RuiyangJu/ThreeStageBinarization" target="_blank">GitHub</a>).</li>
  <li>2023.07: I receive an offer for a Master student at <strong>National Taiwan University</strong>, Taiwan.</li>
  <li>2023.06: One paper is accepted by <strong>Multimedia Tools and Applications</strong> (<a href="https://github.com/Rubbbbbbbbby/SwinOIR" target="_blank">GitHub</a>).</li>
  <li>2022.11: One paper is accepted by <strong>Journal of Real-Time Image Processing</strong> (<a href="https://github.com/RuiyangJu/TripleNet" target="_blank">GitHub</a>).</li>
  <li>2022.10: <strong>ECCV 2022</strong> Competition - Google Universal Image Embedding Challenge <strong>Silver Medal</strong>.</li>
  <li>2022.07: One paper is accepted by <strong>IEEE Access</strong> (<a href="https://github.com/RuiyangJu/ThreshNet" target="_blank">GitHub</a>).</li>
  <li>2021.07: My research project receives a grant from <strong>Tamkang University</strong>.</li>
  <li>2020.12: I join Advanced Mixed-Operation System Laboratory (AMOS Lab.) of <strong>Tamkang University</strong>.</li>
</ul>
</div>

<span class="anchor" id="publication"></span>

# üìÑ Publication
## Kuzushiji Character Recognition
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img  src='https://ruiyangju.github.io/images/Figure/DKDS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization](https://arxiv.org/abs/2511.09117) \\
**Rui-Yang Ju**, Kohei Yamashita, Hirotaka Kameko, Shinsuke Mori \\
[Project](https://ruiyangju.github.io/DKDS/)ÔΩú[GitHub](https://github.com/RuiyangJu/DKDS)
<details>
<summary>Abstract</summary>
Kuzushiji, a pre-modern Japanese cursive script, can currently be read and understood by only a few thousand trained experts in Japan. With the rapid development of deep learning, researchers have begun applying Optical Character Recognition (OCR) techniques to transcribe Kuzushiji into modern Japanese. Although existing OCR methods perform well on clean pre-modern Japanese documents written in Kuzushiji, they often fail to consider various types of noise, such as document degradation and seals, which significantly affect recognition accuracy. To the best of our knowledge, no existing dataset specifically addresses these challenges. To address this gap, we introduce the Degraded Kuzushiji Documents with Seals (DKDS) dataset as a new benchmark for related tasks. We describe the dataset construction process, which required the assistance of a trained Kuzushiji expert, and define two benchmark tracks: (1) text and seal detection and (2) document binarization. For the text and seal detection track, we provide baseline results using multiple versions of the You Only Look Once (YOLO) models for detecting Kuzushiji characters and seals. For the document binarization track, we present baseline results from traditional binarization algorithms, traditional algorithms combined with K-means clustering, and Generative Adversarial Network (GAN)‚Äìbased methods.
</details>

</div>
</div>

## 3D Head Reconstruction
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE VR 2026 Poster</div><img  src='https://ruiyangju.github.io/images/Figure/ToonifyGB.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars](https://arxiv.org/abs/2505.10072) \\
**Rui-Yang Ju**, Sheng-Yen Huang, Yi-Ping Hung \\
[Project](https://ruiyangju.github.io/ToonifyGB)ÔΩú[GitHub](https://github.com/RuiyangJu/ToonifyGB)
<details>
<summary>Abstract</summary>
The introduction of 3D Gaussian blendshapes has enabled the real-time reconstruction of animatable head avatars from monocular video. Toonify, a StyleGAN-based framework, has become widely used for facial image stylization. To extend Toonify for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB. In Stage 1 (stylized video generation), we employ an improved StyleGAN to generate the stylized video from the input video frames, which addresses the limitation of cropping aligned faces at a fixed resolution as preprocessing for normal StyleGAN. This process provides a more stable video, which enables Gaussian blendshapes to better capture the high-frequency details of the video frames, and efficiently generate high-quality animation in the next stage. In Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head model and a set of expression blendshapes from the generated video. By combining the neutral head model with expression blendshapes, ToonifyGB can efficiently render stylized avatars with arbitrary expressions. We validate the effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane and Pixar.
</details>

</div>
</div>

<div class="paper-box-related" markdown="1">

- ``IEEE VR 2026 Poster`` [GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars](https://arxiv.org/abs/2601.17088), **Rui-Yang Ju**, Jen-Shiun Chiang.

</div>

<hr style="margin-top: 20px; margin-bottom: 20px;">
## Document Binarization
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img  src='https://ruiyangju.github.io/images/Figure/document-binarization.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MFE-GAN: Efficient GAN-based Framework for Document Image Enhancement and Binarization with Multi-scale Feature Extraction](https://arxiv.org/abs/2512.14114) \\
**Rui-Yang Ju**, KokSheik Wong, Yanlin Jin, Jen-Shiun Chiang \\
[Project](https://ruiyangju.github.io/MFE-GAN) | [GitHub](https://github.com/RuiyangJu/Efficient_Document_Image_Binarization)
<details>
<summary>Abstract</summary>
Document image enhancement and binarization are commonly performed before document analysis and recognition tasks to improve the efficiency and accuracy of techniques such as optical character recognition (OCR). This is because directly recognizing text in degraded documents, particularly in color images, often obtains unsatisfactory results. Training independent generative adversarial networks (GANs) for each color channel can generate images where shadows and noise are effectively removed, which in turn facilitates efficient text information extraction. However, employing multiple GANs for different color channels requires long training and inference times. To reduce both training and inference times of models for document image enhancement and binarization, we propose MFE-GAN, an efficient GAN-based framework with multi-scale feature extraction (MFE), which incorporates Haar wavelet transformation (HWT) and normalization to process document images before feeding them into GANs for training. In addition, we present novel generators, discriminators, and loss functions to improve the model's performance, and conduct ablation studies to demonstrate their effectiveness. Experimental results on the Benchmark, Nabuco, and CMATERdb datasets show that the proposed MFE-GAN significantly reduces both the total training and inference times while maintaining comparable performance in comparison to state-of-the-art methods.
</details>

</div>
</div>

<div class="paper-box-related" markdown="1">

- ``APSIPA ASC 2025`` [Efficient Generative Adversarial Networks for Color Document Image Enhancement and Binarization Using Multi-scale Feature Extraction](https://ieeexplore.ieee.org/document/11249173), **Rui-Yang Ju**, KokSheik Wong, Jen-Shiun Chiang.
- ``Knowledge-Based Systems 2024`` [Three-stage Binarization of Color Document Images Based on Discrete Wavelet Transform and Generative Adversarial Networks](https://doi.org/10.1016/j.knosys.2024.112542), **Rui-Yang Ju**, Yu-Shian Lin, Yanlin Jin, Chih-Chia Chen, Chun-Tse Chien, Jen-Shiun Chiang.
- ``PRICAI 2023`` [CCDWT-GAN: Generative Adversarial Networks Based on Color Channel Using Discrete Wavelet Transform for Document Image Binarization](https://doi.org/10.1007/978-981-99-7019-3_19), **Rui-Yang Ju**, Yu-Shian Lin, Jen-Shiun Chiang, Chih-Chia Chen, Wei-Han Chen, Chun-Tse Chien.

</div>

<hr style="margin-top: 20px; margin-bottom: 20px;">

## Fracture Detection
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Scientific Reports 2023</div><img src='https://ruiyangju.github.io/images/Figure/fracture-detection.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Fracture Detection in Pediatric Wrist Trauma X-ray Images Using YOLOv8 Algorithm](https://doi.org/10.1038/s41598-023-47460-7) \\
**Rui-Yang Ju**, Weiming Cai \\
[Project](https://ruiyangju.github.io/GRAZPEDWRI-DX_JU/) | [GitHub](https://github.com/RuiyangJu/Bone_Fracture_Detection_YOLOv8)
<details>
<summary>Abstract</summary>
Hospital emergency departments frequently receive lots of bone fracture cases, with pediatric wrist trauma fracture accounting for the majority of them. Before pediatric surgeons perform surgery, they need to ask patients how the fracture occurred and analyze the fracture situation by interpreting X-ray images. The interpretation of X-ray images often requires a combination of techniques from radiologists and surgeons, which requires time-consuming specialized training. With the rise of deep learning in the field of computer vision, network models applying for fracture detection has become an important research topic. In this paper, we use data augmentation to improve the model performance of YOLOv8 algorithm (the latest version of You Only Look Once) on a pediatric wrist trauma X-ray dataset (GRAZPEDWRI-DX), which is a public dataset. The experimental results show that our model has reached the state-of-the-art (SOTA) mean average precision (mAP 50). Specifically, mAP 50 of our model is 0.638, which is significantly higher than the 0.634 and 0.636 of the improved YOLOv7 and original YOLOv8 models. To enable surgeons to use our model for fracture detection on pediatric wrist trauma X-ray images, we have designed the application ‚ÄúFracture Detection Using YOLOv8 App‚Äù to assist surgeons in diagnosing fractures, reducing the probability of error analysis, and providing more useful information for surgery.
</details>

</div>
</div>

<div class="paper-box-related" markdown="1">

- ``IET Image Processing 2025`` [Pediatric Wrist Fracture Detection Using Feature Context Excitation Modules in X-ray Images](https://doi.org/10.1049/ipr2.70269), **Rui-Yang Ju**, Chun-Tse Chien, Enkaer Xieerke, Jen-Shiun Chiang.
- ``IEEE Access 2025`` [YOLOv8-AM: YOLOv8 Based on Effective Attention Mechanisms for Pediatric Wrist Fracture Detection](https://ieeexplore.ieee.org/document/10918980), Chun-Tse Chien, **Rui-Yang Ju**, Kuang-Yi Chou, Enkaer Xieerke, Jen-Shiun Chiang.
- ``Electronics Letters 2024`` [YOLOv9 for Fracture Detection in Pediatric Wrist Trauma X-ray Images](http://dx.doi.org/10.1049/ell2.13248), Chun-Tse Chien, **Rui-Yang Ju**, Kuang-Yi Chou, Jen-Shiun Chiang.
- ``ICONIP 2024`` [YOLOv8-ResCBAM: YOLOv8 Based on An Effective Attention Module for Pediatric Wrist Fracture Detection](https://doi.org/10.1007/978-981-96-6972-1_28), **Rui-Yang Ju**, Chun-Tse Chien, Jen-Shiun Chiang.

</div>

<hr style="margin-top: 20px; margin-bottom: 20px;">

## Visual Odometry
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2025</div><img src='https://ruiyangju.github.io/images/Figure/ORB.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ORB-SfMLearner: ORB-Guided Self-supervised Visual Odometry with Selective Online Adaptation](https://doi.org/10.1109/ICRA55743.2025.11127848) \\
Yanlin Jin, **Rui-Yang Ju**, Haojun Liu, Yuzhong Zhong \\
[Project](https://www.neiljin.site/projects/orbsfm/) | [GitHub](https://github.com/PeaceNeil/ORB-SfMLearner)
<details>
<summary>Abstract</summary>
Deep visual odometry, despite extensive research, still faces limitations in accuracy and generalizability that prevent its broader application. To address these challenges, we propose an Oriented FAST and Rotated BRIEF (ORB)-guided visual odometry with selective online adaptation named ORB-SfMLearner. We present a novel use of ORB features for learning-based ego-motion estimation, leading to more robust and accurate results. We also introduce the cross-attention mechanism to enhance the explainability of PoseNet and have revealed that driving direction of the vehicle can be explained through the attention weights. To improve generalizability, our selective online adaptation allows the network to rapidly and selectively adjust to the optimal parameters across different domains. Experimental results on KITTI and vKITTI datasets show that our method outperforms previous state-of-the-art deep visual odometry methods in terms of ego-motion accuracy and generalizability.
</details>

</div>
</div>

## Super Resolution 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Multimedia Tools and Applications 2023</div><img src='https://ruiyangju.github.io/images/Figure/SwinOIR.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Resolution enhancement processing on low quality images using swin transformer based on interval dense connection strategy](https://doi.org/10.1007/s11042-023-16088-0) \\
**Rui-Yang Ju**, Chih-Chia Chen, Jen-Shiun Chiang, Yu-Shian Lin, Wei-Han Chen, Chun-Tse Chien \\
[GitHub](https://github.com/Rubbbbbbbbby/SwinOIR)
<details>
<summary>Abstract</summary>
The Transformer-based method has demonstrated remarkable performance for image super-resolution in comparison to the method based on the convolutional neural networks (CNNs). However, using the self-attention mechanism like SwinIR (Image Restoration Using Swin Transformer) to extract feature information from images needs a significant amount of computational resources, which limits its application on low computing power platforms. To improve the model feature reuse, this research work proposes the Interval Dense Connection Strategy, which connects different blocks according to the newly designed algorithm. We apply this strategy to SwinIR and present a new model, which named SwinOIR (Object Image Restoration Using Swin Transformer). For image super-resolution, an ablation study is conducted to demonstrate the positive effect of the Interval Dense Connection Strategy on the model performance. Furthermore, we evaluate our model on various popular benchmark datasets, and compare it with other state-of-the-art (SOTA) lightweight models. For example, SwinOIR obtains a PSNR of 26.62 dB for x4 upscaling image super-resolution on Urban100 dataset, which is 0.15 dB higher than the SOTA model SwinIR. For real-life application, this work applies the lastest version of You Only Look Once (YOLOv8) model and the proposed model to perform object detection and real-life image super-resolution on low-quality images.
</details>

</div>
</div>

<span class="anchor" id="professional-service"></span>

# üõ†Ô∏è Professional Service
## Journal Reviewer
<details>
<summary>View more</summary>
<ul>
  <li>IEEE Transactions on Visualization and Computer Graphics.</li>
  <li>Pattern Recognition.</li>
  <li>Knowledge-based Systems.</li>
  <li>Neurocomputing.</li>
  <li>Neural Networks.</li>
  <li>IEEE Signal Processing Letters.</li>
  <li>Engineering Applications of Artificial Intelligence.</li>
  <li>IET Image Processing</li>
  <li>Alexandria Engineering Journal.</li>
  <li>Computer Vision and Image Understanding.</li>
  <li>Multimedia Tools and Applications.</li>
  <li>The Visual Computer.</li>
  <li>Scientific Reports.</li>
  <li>npj Heritage Science.</li>
  <li>Cognitive Computation.</li>
  <li>International Journal of Multimedia Information Retrieval.</li>
  <li>International Journal of Machine Learning and Cybernetics.</li>
  <li>Plos One.</li>
  <li>Journal of Real-Time Image Processing.</li>
  <li>International Journal of Computational Intelligence Systems.</li>
  <li>The Journal of Supercomputing.</li>
  <li>Frontiers in Computer Science.</li>
  <li>Signal, Image and Video Processing.</li>
  <li>Telecommunication Systems.</li>
  <li>Journal of Engineering.</li>
  <li>Franklin Open.</li>
  <li>Journal of King Saud University Computer and Information Sciences.</li>
  <li>Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization.</li>
</ul>
</details>

## Conference Committee and Reviewer
<details>
<summary>View more</summary>
<ul>
  <li>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 2026, Reviewer.</li>
  <li>AAAI Conference on Artificial Intelligence (AAAI), Singapore, 2026, Reviwer.</li>
  <li>Pacific Rim International Conference on Artificial Intelligence (PRICAI), Wellington, New Zealand, 2025, Program Committee.</li>
  <li>International Joint Conference on Neural Networks (IJCNN), Rome, Italy, 2025, Reviewer.</li>
  <li>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Hyderabad, India, 2025, Reviewer.</li>
  <li>Pacific Rim International Conference on Artificial Intelligence (PRICAI), Kyoto, Japan, 2024, Program Committee.</li>
  <li>British Machine Vision Conference (BMVC), Glasgow, UK, 2024, Reviewer.</li>
  <li>Pacific Rim International Conference on Artificial Intelligence (PRICAI), Jakarta, Indonesia, 2023, Program Committee.</li>
</ul>
</details>

<span class="anchor" id="funding"></span>

# üí∞ Funding
<span class='anchor' id='scholarship'></span>
- Kyoto University DoGS SPRING Fellowship, 2025 - 2028.
- National Taiwan University Graduate Research Assistantship, 2024 - 2025.
- National Taiwan University Scholarship, 2023, 2024, 2025.
- Sino International Business Innovation Association (SIBIA) Scholarship, 2021, 2022, 2024.
- Tamkang University Undergraduate Research Fellowship, 2021 - 2022.
- Tamkang University Scholarship (Top 1% Ranking), 2021, 2022.

<span class="anchor" id="education"></span>

# üéì Education
- 2025.10 - 2028.09, Graduate School of Informatics, Kyoto University, Kyoto, Japan.
- 2023.09 - 2025.06, Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan.
- 2019.09 - 2023.06, Electrical and Computer Engineering, Tamkang University, New Taipei, Taiwan.


<hr style="margin-top: 40px; margin-bottom: 20px;">

<div style="text-align: center; font-size: 13px; color: #888; line-height: 1.8;">
  ¬© Copyright 2025 RuiYang Ju.  
  Powered by RayeRen.  
  Hosted by GitHub Pages.  
  Recommended to use Chrome.
</div>
